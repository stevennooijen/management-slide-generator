{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrating text esssence\n",
    "\n",
    "To get nice matching pictures for a sentence, we need to find the right words to query for in the sentence. This notebook is about finding some way to extract the essence of a sentence. Let's use `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also load the unsplash `get_photo_url()` function from notebook `random-line-random-pic.ipynb` to experiment with our results at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.images.unsplash import Unsplash\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/cipsum.txt','r') as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "# remove newlines, split sentences and strip leading and trailing whitespaces\n",
    "lines = [line.strip() for line in corpus.replace(\"\\n\", \"\").split('.') if line is not '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking this for one random line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = str(np.random.choice(lines))\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `spaCy`'s linguistic features\n",
    "\n",
    "Spacy has a nice feature to determine part of speech tagger to get word dependencies within a sentance. Let's have a look. Documentation can be found [here](https://spacy.io/usage/linguistic-features#pos-tagging)\n",
    "\n",
    "First load a language model and receive and create an nlp object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use part of speech tagger from the language model: for each token in the `doc`, we can print the text and the \"pos underscore\" attribute, the predicted part-of-speech tag.\n",
    "\n",
    "In addition to the part-of-speech tags, we can also predict how the words are related. For example, whether a word is the subject of the sentence or an object. The `dep_` attribute returns the predicted dependency label.\n",
    "\n",
    "The head attribute returns the syntactic head token. You can also think of it as the parent token this word is attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's all better explained when visualising it. \n",
    "\n",
    "Ps. you can ask spacy to explain their abbreviations through `spacy.explain('amod')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun chuncks\n",
    "\n",
    "The above allows us to parse whathever we think we need. However, `spaCy` already does some engineering for you an generated so called noun chuncks.\n",
    "\n",
    "> \"Noun chunks are “base noun phrases” – flat phrases that have a noun as their head. You can think of noun chunks as a noun plus the words describing the noun – for example, “the lavish green grass” or “the world’s largest tech fund”. To get the noun chunks in a document, simply iterate over `Doc.noun_chunks`.\"\n",
    "\n",
    "This is very nice ineed! We just extract those and use it for a picture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n",
    "#     print(chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how a picture would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join([chunk.text for chunk in doc.noun_chunks])\n",
    "\n",
    "print('Sentence:', line)\n",
    "print('Query   :', query)\n",
    "\n",
    "unsplash = Unsplash()\n",
    "Image(url = unsplash.get_photo_url(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might still be too much text. Let's consder the nouns only without their adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ' '.join([chunk.root.text for chunk in doc.noun_chunks])\n",
    "\n",
    "print('Sentence:', line)\n",
    "print('Query   :', query)\n",
    "\n",
    "unsplash = Unsplash()\n",
    "Image(url = unsplash.get_photo_url(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better already!\n",
    "\n",
    "How about we include the ROOT VERB of the sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns = [chunk.root.text for chunk in doc.noun_chunks]\n",
    "verbs = [token.text for token in doc if token.dep_ == 'ROOT' and token.pos_ == 'VERB']\n",
    "query = ' '.join(nouns + verbs)\n",
    "\n",
    "print('Sentence:', line)\n",
    "print('Query   :', query)\n",
    "\n",
    "unsplash = Unsplash()\n",
    "Image(url = unsplash.get_photo_url(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmm..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
